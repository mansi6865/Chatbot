import numpy as np
import nltk
import string
import random

f= open('chatbot.txt', 'r', errors= 'ignore')
doc= f.read()
doc= doc.lower()
nltk.download('punkt') #using the punk tokenizer
nltk.download('wordnet') #using the wordnet dictionary
sent_tokens= nltk.sent_tokenize(doc) #converts document to list of sentences
word_tokens= nltk.word_tokenize(doc) #converts doc to list of words

lemmer= nltk.stem.WordNetLemmatizer()
# we'll use wordnet to resolve potential issues that could arise due to use of punctuation marks or sentences that haven't been 
# broken down into words.
def Lem_tokens(tokens):
    return[lemmer.lemmatize(tokens) for token in tokens]
remove_punct= dict((ord(punct), None) for punct in string.punctuation)
def LemNormalize(text):
    return Lem_tokens(nltk.word_tokenize(text.lower().translate(remove_punct)))
    
greet_inputs= ("hello", "hi", "what's up", "hey", "sup")
greet_response= ("hey", "Hello there", "Hi! Nice to talk to you.")
def greet(sentence):
    for word in sentence.split():
        if word.lower() in greet_inputs:
            return random.choice(greet_response)
            
from sklearn.feature_extraction.text import TfidfVectorizer
# this will check the frequency of a word in the corpus and hence measure its rarity by using idf (inverse document function)
from sklearn.metrics.pairwise import cosine_similarity
#it will normalize the vectors for the machine to understand

def response(user_response):
    bot_response= ''
    TfidfVec= TfidfVectorizer(tokenizer= LemNormalize, stop_words= 'english')
    tfidf= TfidfVec.fit_transform(sent_tokens)
    vals= cosine_similarity(tfidf[-1], tfidf)
    idx= vals.argsort()[0][-2]
    flat= vals.flatten()
    flat.sort()
    req_tfidf= flat[-2]
    if(req_tfidf==0):
        bot_response= bot_response + "I'm sorry, I didn't understand that."
        return bot_response
    else:
        bot_response= bot_response + sent_tokens[idx]
        return bot_response
        
flag= True
print("Elisa: Hi! My name is Elisa and I love to talk! Please type Bye any time you want to exit :)")
while (flag== True):
    user_response= input
    user_response= user_response()
    if(user_response!= 'Bye'):
        if(user_response=='thanks' or user_response== 'thankyou'):
            flag= False
            print("Elisa: You're welcome!")
        else:
            if(greet(user_response)!= None):
                print("Elisa: "+greet(user_response))
            else:
                sent_tokens.append(user_response)
                word_tokens= word_tokens+ nltk.word_tokenize(user_response)
                final_words= list(set(word_tokens))
                print("Elisa:", end="")
                print(response(user_response))
                sent_tokens.remove(user_response)
    else:
        flag= False
        print("Elisa: It was nice talking to you! Take care")
        
        
   
